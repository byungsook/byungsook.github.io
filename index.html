<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Byungsoo Kim</title> <meta name="author" content="Byungsoo Kim"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://byungsook.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%63%6F%6E%74%61%63%74.%62%79%75%6E%67%73%6F%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-4482-8363" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=wvLYpJQAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/byungsook" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/byungsoo" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://youtube.com/@byungsook" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fab fa-youtube"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">www.byungsoo.me<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news</a> </li> <li class="nav-item "> <a class="nav-link" href="/#publications">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Byungsoo</span> Kim </h1> <p class="desc">Senior Software Engineer | Generative AI Technology, NVIDIA</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am currently a Senior Software Engineer developing Generative AI Technology at <a href="https://www.nvidia.com/" rel="external nofollow noopener" target="_blank">NVIDIA</a>. I received my joint PhD from <a href="https://graphics.ethz.ch/" rel="external nofollow noopener" target="_blank">Computer Graphics Lab</a> at <a href="https://www.ethz.ch/en.html" rel="external nofollow noopener" target="_blank">ETH Zurich</a> and <a href="https://studios.disneyresearch.com" rel="external nofollow noopener" target="_blank">Disney Research</a>, under the supervision of <a href="https://graphics.ethz.ch/people/grossm/" rel="external nofollow noopener" target="_blank">Prof. Markus Gross</a> and <a href="https://graphics.ethz.ch/~sobarbar/" rel="external nofollow noopener" target="_blank">Prof. Barbara Solenthaler</a>. My research interests span machine learning for physics-based simulation and visual computing.</p> </div> <div class="news"> <a name="news"> <h2>News</h2> </a> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 14, 2024</th> <td> I’m super excited to share that our paper, “Deep Fluids: A Generative Network for Parameterized Fluid Simulations”, has been selected as a recipient of the <a href="https://www.icbs.cn/site/pages/index/index?pageId=2cd3eaea-5f56-4cb1-9276-5a3a9ca39223" rel="external nofollow noopener" target="_blank">2024 Frontiers of Science Award</a> at the International Congress of Basic Science. </td> </tr> <tr> <th scope="row">Jul 11, 2024</th> <td> Our paper “Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution” is published at ACM Transactions on Graphics. </td> </tr> <tr> <th scope="row">Jul 1, 2024</th> <td> Our paper “Study on Morphometrical Urban Aerodynamic Roughness Multi-Scale Exploration Using LiDAR Remote Sensing” is published at Remote Sensing. </td> </tr> <tr> <th scope="row">Jan 16, 2024</th> <td> I am thrilled to share that I, along with Vinicius C. Azevedo, Raphael Ortiz, and Paul Kanyuk, have been nominated for a <a href="https://www.vesglobal.org/press-releases/ves-announces-nominees-for-22nd-annual-ves-awards/" rel="external nofollow noopener" target="_blank">VES Award</a> in the Emerging Technology category for our collaborative work on Elemental - Volumetric Neural Style Transfer. </td> </tr> <tr> <th scope="row">Aug 9, 2023</th> <td> Our ML Hair and NeuralVDB are featured in <a href="https://youtu.be/7yjNW04gVMw" rel="external nofollow noopener" target="_blank">Omniverse SIGGRAPH 2023 Demo</a>. </td> </tr> <tr> <th scope="row">Jul 7, 2023</th> <td> I gave a talk at POSTECH hosted by Prof. Jisung Park. </td> </tr> <tr> <th scope="row">Jun 22, 2023</th> <td> ETH Zurich posted an <a href="https://ethz.ch/en/news-and-events/eth-news/news/2023/06/how-ai-technology-from-eth-animates-the-fire-creatures-in-the-latest-pixar-cinema-movie.html" rel="external nofollow noopener" target="_blank">article</a> about our technology used in “Elemental”. </td> </tr> <tr> <th scope="row">Mar 5, 2023</th> <td> An animation film “Elemental” by Pixar is released where our Neural Flow Stylization technique is used. Check out this <a href="https://beforesandafters.com/2023/06/30/the-ai-volumetric-and-animation-tools-that-helped-make-pixars-elemental-possible/" rel="external nofollow noopener" target="_blank">post</a>! </td> </tr> <tr> <th scope="row">Dec 13, 2022</th> <td> Our paper “Physics-Informed Neural Corrector for Deformation-based Fluid Control” is conditionally accepted to Eurographics 2023. </td> </tr> <tr> <th scope="row">Aug 16, 2022</th> <td> I joined Simulation Technology Team at NVIDIA led by Ken Museth. </td> </tr> </table> <p><a href="/news/">More ...</a></p> </div> </div> <div class="publications"> <a name="publications"> <h2>Publications</h2> </a> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/nrface.jpg"></div> <div id="park2024nrface" class="col-sm-8"> <div class="title">Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution</div> <div class="author"> Hyojoon Park, Sangeetha Grama Srinivasan, Matthew Cong, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Doyub Kim, Byungsoo Kim, Jonathan Swartz, Ken Museth, Eftychios Sifakis' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>ACM Trans. Graph.,</em> Jul 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3670687" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1145/3670687" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://dl.acm.org/doi/suppl/10.1145/3670687/suppl_file/3670687.supp.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present a neural network-based simulation super-resolution framework that can efficiently and realistically enhance a facial performance produced by a low-cost, real-time physics-based simulation to a level of detail that closely approximates that of a reference-quality off-line simulator with much higher resolution (27 \texttimes element count in our examples) and accurate physical modeling. Our approach is rooted in our ability to construct a training set of paired frames, from the low- and high-resolution simulators respectively, that are in semantic correspondence with each other. We use face animation as an exemplar of such a simulation domain, where creating this semantic congruence is achieved by simply dialing in the same muscle actuation controls and skeletal pose in the two simulators. Our proposed neural network super-resolution framework generalizes from this training set to unseen expressions, compensates for modeling discrepancies between the two simulations due to limited resolution or cost-cutting approximations in the real-time variant, and does not require any semantic descriptors or parameters to be provided as input, other than the result of the real-time simulation. We evaluate the efficacy of our pipeline on a variety of expressive performances and provide comparisons and ablation experiments for plausible variations and alternatives to our proposed scheme. Our code is available at https://github.com/hjoonpark/3d-sim-super-res.git.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/urban.jpg"></div> <div id="an2024urban" class="col-sm-8"> <div class="title">Study on Morphometrical Urban Aerodynamic Roughness Multi-Scale Exploration Using LiDAR Remote Sensing</div> <div class="author"> Seung Man An, <em>Byungsoo Kim</em>, Chaeyeon Yi, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jeong-Hee Eum, Jung-Hun Woo, Wolfgang Wende' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Remote Sensing,</em> 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.mdpi.com/2072-4292/16/13/2418/pdf?version=1720062051" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.mdpi.com/2072-4292/16/13/2418" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This study proposes the use of light detection and ranging (LiDAR) remote sensing (RS) to support morphometric research for estimating the aerodynamic roughness length (z0 ) of building placement on various scales. A LiDAR three-dimensional point cloud (3DPC) data processing graphical user interface (GUI) was developed to explore the z0 and related urban canopy parameters (UCPs) in the Incheon metropolitan area in South Korea. The results show that multi-scale urban aerodynamic roughness exploration is viable and can address differences in urban building data at various spatial resolutions. Although validating morphological multi-scale UCPs using dense tall towers is challenging, emerging low-cost and efficient methods can serve as substitutes. However, further efforts are required to link the measured z0 to building form regulations, such as floor area ratio, and expand RS research to obtain more quantitative and qualitative knowledge.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/phydef.jpg"></div> <div id="tang2023phydef" class="col-sm-8"> <div class="title">Physics-Informed Neural Corrector for Deformation-based Fluid Control</div> <div class="author"> Jingwei Tang, <em>Byungsoo Kim</em>, Vinicius C. Azevedo, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Barbara Solenthaler' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Computer Graphics Forum (Proc. EUROGRAPHICS),</em> May 2023. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://studios.disneyresearch.com/app/uploads/2023/04/Physics_Informed_Neural_Corrector_for_Deformation_based_Fluid_Control-6.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14751" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://www.youtube.com/watch?v=jE00lzetddk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://studios.disneyresearch.com/2023/05/07/physics-informed-neural-corrector-for-deformation-based-fluid-control/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Abstract Controlling fluid simulations is notoriously difficult due to its high computational cost and the fact that user control inputs can cause unphysical motion. We present an interactive method for deformation-based fluid control. Our method aims at balancing the direct deformations of fluid fields and the preservation of physical characteristics. We train convolutional neural networks with physics-inspired loss functions together with a differentiable fluid simulator, and provide an efficient workflow for flow manipulations at test time. We demonstrate diverse test cases to analyze our carefully designed objectives and show that they lead to physical and eventually visually appealing modifications on edited fluid data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="https://studios.disneyresearch.com/app/uploads/2022/07/Implicit_Neural_Representation_for_Physics-driven_Actuated_Soft_Bodies.jpg"></div> <div id="yang2022implicit" class="col-sm-8"> <div class="title">Implicit Neural Representation for Physics-Driven Actuated Soft Bodies</div> <div class="author"> Lingchen Yang, <em>Byungsoo Kim</em>, Gaspard Zoss, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Baran Gözcü, Markus Gross, Barbara Solenthaler' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>ACM Trans. Graph. (Proc. SIGGRAPH),</em> Jul 2022. <b>(Honorable Mention)</b> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://studios.disneyresearch.com/app/uploads/2022/07/Implicit_Neural_Representation_for_Physics-driven_Actuated_Soft_Bodies_final-1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://studios.disneyresearch.com/app/uploads/2022/08/Implicit_Neural_Representation_for_Physics-driven_Actuated_Soft_Bodies_supplemental.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://doi.org/10.1145/3528223.3530156" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://www.youtube.com/watch?v=9EERe_CTazk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://studios.disneyresearch.com/2022/07/24/implicit-neural-representation-for-physics-driven-actuated-soft-bodies/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal’s dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/sketchden.jpg"></div> <div id="kim2022sketch" class="col-sm-8"> <div class="title">Deep Reconstruction of 3D Smoke Densities from Artist Sketches</div> <div class="author"> <em>Byungsoo Kim</em>, Xingchang Huang, Laura Wuelfroth, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Jingwei Tang, Guillaume Cordonnier, Markus Gross, Barbara Solenthaler' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Computer Graphics Forum (Proc. EUROGRAPHICS),</em> May 2022. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://cgl.ethz.ch/Downloads/Publications/Papers/2022/Kim22a/Kim22a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://cgl.ethz.ch/Downloads/Publications/Papers/2022/Kim22a/Kim22a_supp.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/byungsook/sketch2fluid" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/sketchden.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14461" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://www.youtube.com/watch?v=PF7QqNZ28hk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Abstract Creative processes of artists often start with hand-drawn sketches illustrating an object. Pre-visualizing these keyframes is especially challenging when applied to volumetric materials such as smoke. The authored 3D density volumes must capture realistic flow details and turbulent structures, which is highly non-trivial and remains a manual and time-consuming process. We therefore present a method to compute a 3D smoke density field directly from 2D artist sketches, bridging the gap between early-stage prototyping of smoke keyframes and pre-visualization. From the sketch inputs, we compute an initial volume estimate and optimize the density iteratively with an updater CNN. Our differentiable sketcher is embedded into the end-to-end training, which results in robust reconstructions. Our training data set and sketch augmentation strategy are designed such that it enables general applicability. We evaluate the method on synthetic inputs and sketches from artists depicting both realistic smoke volumes and highly non-physical smoke shapes. The high computational performance and robustness of our method at test time allows interactive authoring sessions of volumetric density fields for rapid prototyping of ideas by novice users.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iceflow.jpg"></div> <div id="jouvet2022igm" class="col-sm-8"> <div class="title">Deep Learning Speeds Up Ice Flow Modelling by Several Orders of Magnitude</div> <div class="author"> Guillaume Jouvet, Guillaume Cordonnier, <em>Byungsoo Kim</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Martin Lüthi, Andreas Vieli, Andy Aschwanden' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Journal of Glaciology,</em> Dec 2021. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/523822/3/deep-learning-speeds-up-ice-flow-modelling-by-several-orders-of-magnitude.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1017/jog.2021.120" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper introduces the Instructed Glacier Model (IGM) – a model that simulates ice dynamics, mass balance and its coupling to predict the evolution of glaciers, icefields or ice sheets. The novelty of IGM is that it models the ice flow by a Convolutional Neural Network, which is trained from data generated with hybrid SIA + SSA or Stokes ice flow models. By doing so, the most computationally demanding model component is substituted by a cheap emulator. Once trained with representative data, we demonstrate that IGM permits to model mountain glaciers up to 1000 × faster than Stokes ones on Central Processing Units (CPU) with fidelity levels above 90% in terms of ice flow solutions leading to nearly identical transient thickness evolution. Switching to the GPU often permits additional significant speed-ups, especially when emulating Stokes dynamics or/and modelling at high spatial resolution. IGM is an open-source Python code which deals with two-dimensional (2-D) gridded input and output data. Together with a companion library of trained ice flow emulators, IGM permits user-friendly, highly efficient and mechanically state-of-the-art glacier and icefields simulations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#98012E"><a href="https://www.ieee-ras.org/publications/t-ro" rel="external nofollow noopener" target="_blank">T-RO</a></abbr></div> <div id="Charreyron2021emns" class="col-sm-8"> <div class="title">Modeling Electromagnetic Navigation Systems</div> <div class="author"> Samuel L. Charreyron, Quentin Boehler, <em>Byungsoo Kim</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Cameron Weibel, Christophe Chautems, Bradley J. Nelson' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Robotics,</em> Jan 2021. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/468301/Modeling_Electromagnetic_Navigation_Systems.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://doi.org/10.1109/TRO.2020.3047053" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Remote magnetic navigation is used for the manipulation of untethered micro and nanorobots, as well as tethered magnetic surgical tools for minimally invasive medicine. Mathematical modeling of the magnetic fields generated by magnetic navigation systems is a fundamental task in the control of such tools for biomedical applications. In this article, we describe and compare several existing and newly developed methods for representations of continuous magnetic fields using interpolation in the context of remote magnetic navigation. Clinical-scale electromagnetic navigation systems feature nonlinear magnetization and magnetization interactions between electromagnets, which renders accurate magnetic field modeling challenging. We first introduce a method that can adapt existing linear models to correct for nonlinear magnetization, with similar performance to the current state-of-the-art nonlinear model. Furthermore, we present a method based on convolutional neural networks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/phd_thesis.png"></div> <div id="kim2020phdthesis" class="col-sm-8"> <div class="title">Data-Driven Methods for Artist-Directed Fluid Simulations</div> <div class="author"><em>Byungsoo Kim</em></div> <em>Doctoral Thesis</em>. ETH Zurich. 2020. <div class="links"> <a href="https://doi.org/10.3929/ethz-b-000468169" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/latent.jpg"></div> <div id="wiewel2020latent" class="col-sm-8"> <div class="title">Latent Space Subdivision: Stable and Controllable Time Predictions for Fluid Flow</div> <div class="author"> Steffen Wiewel, <em>Byungsoo Kim</em>, Vinicius C. Azevedo, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Barbara Solenthaler, Nils Thuerey' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Computer Graphics Forum (Proc. SCA),</em> Nov 2020. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2003.08723" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14097" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://ge.in.tum.de/publications/2020-lssubdiv-wiewel/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Abstract We propose an end-to-end trained neural network architecture to robustly predict the complex dynamics of fluid flows with high temporal stability. We focus on single-phase smoke simulations in 2D and 3D based on the incompressible Navier-Stokes (NS) equations, which are relevant for a wide range of practical problems. To achieve stable predictions for long-term flow sequences with linear execution times, a convolutional neural network (CNN) is trained for spatial compression in combination with a temporal prediction network that consists of stacked Long Short-Term Memory (LSTM) layers. Our core contribution is a novel latent space subdivision (LSS) to separate the respective input quantities into individual parts of the encoded latent space domain. As a result, this allows to distinctively alter the encoded quantities without interfering with the remaining latent space values and hence maximizes external control. By selectively overwriting parts of the predicted latent space points, our proposed method is capable to robustly predict long-term sequences of complex physics problems, like the flow of fluids. In addition, we highlight the benefits of a recurrent training on the latent space creation, which is performed by the spatial compression network. Furthermore, we thoroughly evaluate and discuss several different components of our method.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/lnst.jpg"></div> <div id="kim2020lnst" class="col-sm-8"> <div class="title">Lagrangian Neural Style Transfer for Fluids</div> <div class="author"> <em>Byungsoo Kim</em>, Vinicius C. Azevedo, Markus Gross, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Barbara Solenthaler' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>ACM Trans. Graph. (Proc. SIGGRAPH),</em> Jul 2020. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2005.00803" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/byungsook/neural-flow-style" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.1145/3386569.3392473" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://www.youtube.com/watch?v=WPmUsIVf3-4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Artistically controlling the shape, motion and appearance of fluid simulations pose major challenges in visual effects production. In this paper, we present a neural style transfer approach from images to 3D fluids formulated in a Lagrangian viewpoint. Using particles for style transfer has unique benefits compared to grid-based techniques. Attributes are stored on the particles and hence are trivially transported by the particle motion. This intrinsically ensures temporal consistency of the optimized stylized structure and notably improves the resulting quality. Simultaneously, the expensive, recursive alignment of stylization velocity fields of grid approaches is unnecessary, reducing the computation time to less than an hour and rendering neural flow stylization practical in production settings. Moreover, the Lagrangian representation improves artistic control as it allows for multi-fluid stylization and consistent color transfer from images, and the generality of the method enables stylization of smoke and liquids likewise.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/cnst.jpg"></div> <div id="christen2020cnst" class="col-sm-8"> <div class="title">Neural Smoke Stylization with Color Transfer</div> <div class="author"> Fabienne Christen, <em>Byungsoo Kim</em>, Vinicius C. Azevedo, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Barbara Solenthaler' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Eurographics 2020 - Short Papers,</em> May 2020. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1912.08757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.youtube.com/watch?v=CwvuaQBnJbA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> <a href="https://doi.org/10.2312/egs.20201015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://www.youtube.com/watch?v=TyNlaBoP6oI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Artistically controlling fluid simulations requires a large amount of manual work by an artist. The recently presented transportbased neural style transfer approach simplifies workflows as it transfers the style of arbitrary input images onto 3D smoke simulations. However, the method only modifies the shape of the fluid but omits color information. In this work, we therefore extend the previous approach to obtain a complete pipeline for transferring shape and color information onto 2D and 3D smoke simulations with neural networks. Our results demonstrate that our method successfully transfers colored style features consistently in space and time to smoke data for different input textures.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/deepfreq.jpg"></div> <div id="biland2020deepfreq" class="col-sm-8"> <div class="title">Frequency-Aware Reconstruction of Fluid Simulations with Generative Networks</div> <div class="author"> Simon Biland, Vinicius C. Azevedo, <em>Byungsoo Kim</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Barbara Solenthaler' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Eurographics 2020 - Short Papers,</em> May 2020. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1912.08776" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.youtube.com/watch?v=yR7INFmFLDY&amp;t=180s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> <a href="https://doi.org/10.2312/egs.20201019" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Convolutional neural networks were recently employed to fully reconstruct fluid simulation data from a set of reduced parameters. However, since (de-)convolutions traditionally trained with supervised L1-loss functions do not discriminate between low and high frequencies in the data, the error is not minimized efficiently for higher bands. This directly correlates with the quality of the perceived results, since missing high frequency details are easily noticeable. In this paper, we analyze the reconstruction quality of generative networks and present a frequency-aware loss function that is able to focus on specific bands of the dataset during training time. We show that our approach improves reconstruction quality of fluid simulation data in mid-frequency bands, yielding perceptually better results while requiring comparable training time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/tnst.jpg"></div> <div id="kim2019tnst" class="col-sm-8"> <div class="title">Transport-Based Neural Style Transfer for Smoke Simulations</div> <div class="author"> <em>Byungsoo Kim</em>, Vinicius C. Azevedo, Markus Gross, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Barbara Solenthaler' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>ACM Trans. Graph. (Proc. SIGGRAPH Asia),</em> Nov 2019. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1905.07442" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/byungsook/neural-flow-style" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/tnst.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://doi.org/10.1145/3355089.3356560" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://www.youtube.com/watch?v=67qVRhoOQPE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Artistically controlling fluids has always been a challenging task. Optimization techniques rely on approximating simulation states towards target velocity or density field configurations, which are often handcrafted by artists to indirectly control smoke dynamics. Patch synthesis techniques transfer image textures or simulation features to a target flow field. However, these are either limited to adding structural patterns or augmenting coarse flows with turbulent structures, and hence cannot capture the full spectrum of different styles and semantically complex structures. In this paper, we propose the first Transport-based Neural Style Transfer (TNST) algorithm for volumetric smoke data. Our method is able to transfer features from natural images to smoke simulations, enabling general content-aware manipulations ranging from simple patterns to intricate motifs. The proposed algorithm is physically inspired, since it computes the density transport from a source input smoke to a desired target configuration. Our transport-based approach allows direct control over the divergence of the stylization velocity field by optimizing incompressible and irrotational potentials that transport smoke towards stylization. Temporal consistency is ensured by transporting and aligning subsequent stylized velocities, and 3D reconstructions are computed by seamlessly merging stylizations from different camera viewpoints.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/deepvortices.jpg"></div> <div id="kim2019deepvortices" class="col-sm-8"> <div class="title">Robust Reference Frame Extraction from Unsteady 2D Vector Fields with Convolutional Neural Networks</div> <div class="author"> <em>Byungsoo Kim</em>, and Tobias Günther</div> <div class="periodical"> <em>Computer Graphics Forum (Proc. EUROVIS),</em> Jul 2019. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1903.10255" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/deepvortices.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13689" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Abstract Robust feature extraction is an integral part of scientific visualization. In unsteady vector field analysis, researchers recently directed their attention towards the computation of near-steady reference frames for vortex extraction, which is a numerically challenging endeavor. In this paper, we utilize a convolutional neural network to combine two steps of the visualization pipeline in an end-to-end manner: the filtering and the feature extraction. We use neural networks for the extraction of a steady reference frame for a given unsteady 2D vector field. By conditioning the neural network to noisy inputs and resampling artifacts, we obtain numerically stabler results than existing optimization-based approaches. Supervised deep learning typically requires a large amount of training data. Thus, our second contribution is the creation of a vector field benchmark data set, which is generally useful for any local deep learning-based feature extraction. Based on Vatistas velocity profile, we formulate a parametric vector field mixture model that we parameterize based on numerically-computed example vector fields in near-steady reference frames. Given the parametric model, we can efficiently synthesize thousands of vector fields that serve as input to our deep learning architecture. The proposed network is evaluated on an unseen numerical fluid flow simulation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/deepfluids.jpg"></div> <div id="kim2019deepfluids" class="col-sm-8"> <div class="title">Deep Fluids: A Generative Network for Parameterized Fluid Simulations</div> <div class="author"> <em>Byungsoo Kim</em>, Vinicius C. Azevedo, Nils Thuerey, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Theodore Kim, Markus Gross, Barbara Solenthaler' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Computer Graphics Forum (Proc. EUROGRAPHICS),</em> Jun 2019. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1806.02071" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/byungsook/deep-fluids" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/deepfluids.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://www.youtube.com/watch?v=hSDzOZ9IO8U" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Abstract This paper presents a novel generative model to synthesize fluid simulations from a set of reduced parameters. A convolutional neural network is trained on a collection of discrete, parameterizable fluid simulation velocity fields. Due to the capability of deep learning architectures to learn representative features of the data, our generative model is able to accurately approximate the training data set, while providing plausible interpolated in-betweens. The proposed generative model is optimized for fluids by a novel loss function that guarantees divergence-free velocity fields at all times. In addition, we demonstrate that we can handle complex parameterizations in reduced spaces, and advance simulations in time by integrating in the latent space with a second network. Our method models a wide variety of fluid behaviors, thus enabling applications such as fast construction of simulations, interpolation of fluids with different parameters, time re-sampling, latent space simulations, and compression of fluid simulation data. Reconstructed velocity fields are generated up to 700× faster than re-simulating the data with the underlying CPU solver, while achieving compression rates of up to 1300×.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/vectornet.jpg"></div> <div id="kim2018semantic" class="col-sm-8"> <div class="title">Semantic Segmentation for Line Drawing Vectorization Using Neural Networks</div> <div class="author"> <em>Byungsoo Kim</em>, Oliver Wang, A. Cengiz Öztireli, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Markus Gross' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Computer Graphics Forum (Proc. EUROGRAPHICS),</em> May 2018. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://cgl.ethz.ch/Downloads/Publications/Papers/2018/Kim18a/Kim18a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/byungsook/vectornet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/vectornet.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13365" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Abstract In this work, we present a method to vectorize raster images of line art. Inverting the rasterization procedure is inherently ill-conditioned, as there exist many possible vector images that could yield the same raster image. However, not all of these vector images are equally useful to the user, especially if performing further edits is desired. We therefore define the problem of computing an instance segmentation of the most likely set of paths that could have created the raster image. Once the segmentation is computed, we use existing vectorization approaches to vectorize each path, and then combine all paths into the final output vector image. To determine which set of paths is most likely, we train a pair of neural networks to provide semantic clues that help resolve ambiguities at intersection and overlap regions. These predictions are made considering the full context of the image, and are then globally combined by solving a Markov Random Field (MRF). We demonstrate the flexibility of our method by generating results on character datasets, a synthetic random line dataset, and a dataset composed of human drawn sketches. For all cases, our system accurately recovers paths that adhere to the semantics of the drawings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/master_thesis.png"></div> <div id="kim2016masterthesis" class="col-sm-8"> <div class="title">Learning Structured Representations for Geometry</div> <div class="author"><em>Byungsoo Kim</em></div> <em>Master Thesis</em>. ETH Zurich. 2016. <div class="links"> <a href="https://doi.org/10.3929/ethz-a-010807297" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/old_pub.png"></div> <div id="kim2014oldpub" class="col-sm-8"> <div class="title">Previous Works</div> <div class="author"><em>Byungsoo Kim</em></div>2008 - 2014. <div class="links"> <a href="https://sites.google.com/site/byungsookim0608" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> </div> <div class="badges"> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Byungsoo Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>