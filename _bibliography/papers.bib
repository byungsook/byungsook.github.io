@article{yang2022implicit,
author = {Yang, Lingchen and Kim, Byungsoo and Zoss, Gaspard and G\"{o}zc\"{u}, Baran and Gross, Markus and Solenthaler, Barbara},
title = {Implicit Neural Representation for Physics-Driven Actuated Soft Bodies},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3528223.3530156},
html = {https://studios.disneyresearch.com/2022/07/24/implicit-neural-representation-for-physics-driven-actuated-soft-bodies/},
pdf = {https://studios.disneyresearch.com/app/uploads/2022/07/Implicit_Neural_Representation_for_Physics-driven_Actuated_Soft_Bodies_final-1.pdf},
doi = {10.1145/3528223.3530156},
abstract = {Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {122},
numpages = {10},
keywords = {differentiable physics, digital human, deep learning},
selected={true}
}